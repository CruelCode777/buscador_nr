import streamlit as st
import osÂ  # <--- Importante adicionar isso
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import ChatPromptTemplate

# ConfiguraÃ§Ã£o da PÃ¡gina
st.set_page_config(page_title="IA de SeguranÃ§a do Trabalho", page_icon="ğŸ‘·", layout="centered")

# --- SEGREDOS ---
groq_key = st.secrets["GROQ_API_KEY"]
pinecone_key = st.secrets["PINECONE_API_KEY"]

st.title("ğŸ‘· Consultor de NRs (IA)")
st.caption("Base de conhecimento unificada de todas as Normas Regulamentadoras.")

# --- CONEXÃƒO COM A BASE DE DADOS (PINECONE) ---
@st.cache_resource
def get_knowledge_base():
Â  Â  # Define a chave no ambiente (Ã© assim que a nova biblioteca procura)
Â  Â  os.environ['PINECONE_API_KEY'] = pinecone_keyÂ 

Â  Â  embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
Â  Â Â 
Â  Â  # Conecta ao Ã­ndice (agora sem passar a chave explicitamente aqui dentro)
Â  Â  vectorstore = PineconeVectorStore.from_existing_index(
Â  Â  Â  Â  index_name="base-nrs",
Â  Â  Â  Â  embedding=embeddings
Â  Â  )
Â  Â  return vectorstore

try:
Â  Â  vectorstore = get_knowledge_base()
except Exception as e:
Â  Â  st.error(f"Erro ao conectar no banco de dados: {e}")
Â  Â  st.stop()

# --- CHAT ---
if "messages" not in st.session_state:
Â  Â  st.session_state.messages = []

# Mostra histÃ³rico
for message in st.session_state.messages:
Â  Â  with st.chat_message(message["role"]):
Â  Â  Â  Â  st.markdown(message["content"])

# Campo de pergunta
if prompt := st.chat_input("Ex: Quais os exames obrigatÃ³rios para trabalho em altura?"):
Â  Â  st.session_state.messages.append({"role": "user", "content": prompt})
Â  Â  with st.chat_message("user"):
Â  Â  Â  Â  st.markdown(prompt)

Â  Â  with st.chat_message("assistant"):
Â  Â  Â  Â  with st.spinner("Consultando a base unificada de normas..."):
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  # 1. Busca os trechos mais relevantes no Pinecone
Â  Â  Â  Â  Â  Â  Â  Â  retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
Â  Â  Â  Â  Â  Â  Â  Â  docs = retriever.invoke(prompt)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if not docs:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response_text = "NÃ£o encontrei informaÃ§Ãµes sobre isso na base de dados das NRs."
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Formata o contexto
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  context_text = ""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sources = set()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for doc in docs:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ProteÃ§Ã£o caso o metadado 'source' esteja vazio
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  src = doc.metadata.get('source', 'Desconhecido')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  context_text += f"{doc.page_content}\n(Fonte: {src})\n---\n"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sources.add(src)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # 2. O Prompt
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  system_prompt = """
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  VocÃª Ã© um Consultor SÃªnior em SeguranÃ§a do Trabalho (HSE).
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Sua missÃ£o Ã© orientar profissionais com base estrita nas Normas Regulamentadoras (NRs).
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Diretrizes:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  1. Use tÃ³picos para listas.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  2. Cite qual NR e item embasa a resposta.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  3. Se nÃ£o estiver no contexto, diga que a norma nÃ£o especifica.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Contexto das Normas:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {context}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Pergunta do UsuÃ¡rio: {question}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prompt_template = ChatPromptTemplate.from_template(system_prompt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # 3. Chama a IA (Groq) - Usando modelo estÃ¡vel
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llm = ChatGroq(temperature=0.1, model_name="llama-3.3-70b-versatile", groq_api_key=groq_key)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chain = prompt_template | llm
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response = chain.invoke({"context": context_text, "question": prompt})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response_text = response.content + f"\n\n\n*Fontes consultadas: {', '.join(sources)}*"
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(response_text)
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role": "assistant", "content": response_text})
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Ocorreu um erro durante a resposta: {e}")
