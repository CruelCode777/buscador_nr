import streamlit as st
import streamlit_analytics2 as streamlit_analytics
import os
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import ChatPromptTemplate

# --- CONFIGURA√á√ÉO DA P√ÅGINA  ---
st.set_page_config(page_title="Consultor SST", page_icon="üë∑", layout="centered")

# --- IN√çCIO DO RASTREAMENTO ---

with streamlit_analytics.track():

    # --- SEGREDOS ---
    groq_key = st.secrets["GROQ_API_KEY"]
    pinecone_key = st.secrets["PINECONE_API_KEY"]

    st.title("üë∑ Consultor de NRs")
    st.caption("Base de conhecimento unificada de todas as Normas Regulamentadoras GOV.BR database 2026")

    # --- CONEX√ÉO COM A BASE DE DADOS (PINECONE) ---
    @st.cache_resource
    def get_knowledge_base():
        # Define a chave no ambiente
        os.environ['PINECONE_API_KEY'] = pinecone_key 

        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        
        vectorstore = PineconeVectorStore.from_existing_index(
            index_name="base-nrs",
            embedding=embeddings
        )
        return vectorstore

    try:
        vectorstore = get_knowledge_base()
    except Exception as e:
        st.error(f"Erro ao conectar no banco de dados: {e}")
        st.stop()

    # --- CHAT ---
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Mostra hist√≥rico
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Campo de pergunta
    if prompt := st.chat_input("Ex: Quais os exames obrigat√≥rios para trabalho em altura?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Consultando a base unificada de normas..."):
                
                try:
                    # 1. Busca os trechos mais relevantes no Pinecone
                    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
                    docs = retriever.invoke(prompt)
                    
                    if not docs:
                        response_text = "N√£o encontrei informa√ß√µes sobre isso na base de dados das NRs."
                    else:
                        # Formata o contexto
                        context_text = ""
                        sources = set()
                        for doc in docs:
                            src = doc.metadata.get('source', 'Desconhecido')
                            context_text += f"{doc.page_content}\n(Fonte: {src})\n---\n"
                            sources.add(src)

                        # 2. O Prompt
                        system_prompt = """
                        Voc√™ √© um Consultor S√™nior em Seguran√ßa do Trabalho (HSE).
                        Sua miss√£o √© orientar profissionais com base estrita nas Normas Regulamentadoras (NRs).
                        
                        Diretrizes:
                        1. Use t√≥picos para listas.
                        2. Cite qual NR e item embasa a resposta.
                        3. Se n√£o estiver no contexto, diga que a norma n√£o especifica.
                        
                        Contexto das Normas:
                        {context}
                        
                        Pergunta do Usu√°rio: {question}
                        """
                        
                        prompt_template = ChatPromptTemplate.from_template(system_prompt)
                        
                        # 3. Chama a IA (Groq)
                        llm = ChatGroq(temperature=0.1, model_name="llama-3.3-70b-versatile", groq_api_key=groq_key)
                        chain = prompt_template | llm
                        
                        response = chain.invoke({"context": context_text, "question": prompt})
                        
                        response_text = response.content + f"\n\n\n*Fontes consultadas: {', '.join(sources)}*"
                    
                    st.markdown(response_text)
                    st.session_state.messages.append({"role": "assistant", "content": response_text})
                
                except Exception as e:
                    st.error(f"Ocorreu um erro durante a resposta: {e}")

# --- FIM DO RASTREAMENTO ---

# --- √ÅREA ADMINISTRATIVA (VIS√çVEL APENAS COM SENHA) ---
st.write("---")
# Escolha uma senha dif√≠cil aqui. O padr√£o abaixo √© "carlos123"
streamlit_analytics.view(password="carlos123")

